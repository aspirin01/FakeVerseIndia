{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b5385a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time \n",
    "import pandas as pd\n",
    "import json\n",
    "import requests\n",
    "from pyquery import PyQuery\n",
    "import numpy as np\n",
    "from requests.adapters import HTTPAdapter, Retry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e3d878d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_links(page_num)\n",
    "    # Extraction of all links till june 2020\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "\n",
    "\n",
    "    #driver = webdriver.Chrome(service=service)\n",
    "    browser = webdriver.Chrome(service=service)\n",
    "    urls = []\n",
    "    articles_link = [\"Links\"]\n",
    "    # since the tops category has 7 pages, where link to each follows a specific pattern identified above, we can create links to them as following:\n",
    "    urls.append('https://hindi.thequint.com/news/webqoof')\n",
    "\n",
    "    for i in range(1, page_num):\n",
    "        urls.append('https://hindi.thequint.com/news/webqoof/' + str(i))\n",
    "\n",
    "\n",
    "    # extracting links for products in each page\n",
    "    for url in urls:\n",
    "        # open the url\n",
    "        browser.get(url)\n",
    "        # purposeful wait time to allow the website to get fully loaded\n",
    "        time.sleep(4)\n",
    "        # get page content\n",
    "        content = browser.page_source\n",
    "        soup = BeautifulSoup(content, \"lxml\")\n",
    "        art_links = []\n",
    "        # extract all the anchor i.e., <a> elements with “thumb-link” class from the page\n",
    "        #data_links = soup.find_all(\"a\", {\"class\": \"headline-link _3aBL6\"})\n",
    "\n",
    "        #data1_links = soup.find_all(\"a\", {\"class\": \"title-wrapper headline-link _3b9Yy\"})\n",
    "\n",
    "        data2_links = soup.find_all(\"a\", {\"class\": \"headline-link\"})\n",
    "        # from each <a> element, extract the URL\n",
    "\n",
    "        for i in data2_links:\n",
    "            art_links.append(i['href'])\n",
    "\n",
    "        articles_link.extend(art_links[10:])\n",
    "        # purposeful wait time to avoid sending requests in quick succession\n",
    "        time.sleep(10)\n",
    "    browser.quit()\n",
    "    np.savetxt(\"Quint_webqoof_hindi_links_june2020.csv\", articles_link, delimiter =\", \", fmt ='% s')\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb26bead",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_content(dt):\n",
    "    for url in dt[\"Links\"]:\n",
    "        #url = \"https://fit.thequint.com/fit-webqoof/drinking-milk-after-eating-fish-dangerous-fact-check\"\n",
    "        session = requests.Session()\n",
    "        retry = Retry(connect=3, backoff_factor=0.5)\n",
    "        adapter = HTTPAdapter(max_retries=retry)\n",
    "        session.mount('http://', adapter)\n",
    "        session.mount('https://', adapter)\n",
    "\n",
    "        response = session.get(url)\n",
    "        # Check if the request was successful\n",
    "        if response.status_code == 200:\n",
    "            linnnks.append(url)\n",
    "            # Parse the HTML content of the page using BeautifulSoup\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            meta_tags = soup.find_all('meta')\n",
    "            meta_features = {}\n",
    "            for tag in meta_tags:\n",
    "                if tag.get('property'):\n",
    "                    meta_features[tag['property']] = tag.get('content')\n",
    "            # title\n",
    "            title.append(meta_features['og:title'])\n",
    "\n",
    "            # Published_date\n",
    "            if 'article:published_time' in meta_features:\n",
    "                Published_date.append(meta_features['article:published_time'])\n",
    "            elif 'article:published' in meta_features:\n",
    "                Published_date.append(meta_features['article:published'])\n",
    "            else:\n",
    "                Published_date.append(\" \")\n",
    "            # top image\n",
    "            top_image.append(meta_features['og:image'])\n",
    "\n",
    "            # Content\n",
    "            data = soup.find_all('div', {'class':\"story-element story-element-text\"})\n",
    "            if data:\n",
    "                extracted_content = []\n",
    "                investigation_content = \"\"\n",
    "                booll = 0\n",
    "                for element in data:\n",
    "                    for el in element.find_all(['p', 'blockquote']):\n",
    "                        extracted_content.append(el.get_text())\n",
    "                        if booll == 0:\n",
    "                            claim.append(el.get_text())\n",
    "                            booll = 1\n",
    "                        else:\n",
    "                            investigation_content += el.get_text() + \"\\n\"\n",
    "\n",
    "                full_content = '\\n'.join(extracted_content)\n",
    "\n",
    "            Content.append(full_content)\n",
    "            investigation.append(investigation_content) \n",
    "\n",
    "            # Extract all links\n",
    "            links = soup.select('div.story-element-text a')\n",
    "            not_include = [\"https://www.thequint.com/subscribe\", \"https://t.me/TheQuint\", \"https://www.thequint.com/news/webqoof\"]\n",
    "            # Extract and print the links\n",
    "            linksss = []\n",
    "            for link in links:\n",
    "                href = link.get('href')\n",
    "                if href not in not_include:\n",
    "                    if link:\n",
    "                        linksss.append(href)\n",
    "\n",
    "            links_in_text.append(linksss)\n",
    "    print(len(title))\n",
    "    print(len(Published_date))\n",
    "    print(len(Content))\n",
    "    print(len(claim))\n",
    "    print(len(investigation))\n",
    "    print(len(top_image))\n",
    "    print(len(links_in_text))\n",
    "    print(len(linnnks) )   \n",
    "    #save to csv file \n",
    "    df1 = pd.DataFrame({'link':linnnks, 'title': title, 'publish_date': Published_date, 'content': Content, 'investigation': investigation, 'Links_in_text':links_in_text,'top image': top_image})\n",
    "    df1.to_csv(\"Quint_english_june2020.csv\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa303c98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3091\n",
      "3091\n",
      "3091\n",
      "3084\n",
      "3091\n",
      "3091\n",
      "3091\n",
      "3091\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    title = []\n",
    "    Published_date = []\n",
    "    Content = []\n",
    "    claim = []\n",
    "    investigation = []\n",
    "    top_image = []\n",
    "    links_in_text = []\n",
    "    linnnks = []\n",
    "    get_links(200)\n",
    "    dataframe = pd.read_csv(\"Quint_webqoof_english_links_june2020.csv\")\n",
    "    get_content(dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca30fce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
