{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b5385a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time \n",
    "import pandas as pd\n",
    "import json\n",
    "import requests\n",
    "from pyquery import PyQuery\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from requests.adapters import HTTPAdapter, Retry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4716bdc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_links(page_num):\n",
    "    from selenium import webdriver\n",
    "    import time\n",
    "    from selenium.common.exceptions import StaleElementReferenceException\n",
    "\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "\n",
    "\n",
    "    driver = webdriver.Chrome(service=service)\n",
    "    driver.maximize_window()\n",
    "    driver.implicitly_wait(10)\n",
    "    driver.get(\"https://www.vishvasnews.com/english/\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    j = 0\n",
    "    try:\n",
    "        while j<page_num:\n",
    "            nav_element = WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.ID, \"nav-link\"))\n",
    "                )\n",
    "\n",
    "            load_more_link = nav_element.find_element(By.PARTIAL_LINK_TEXT, \"Load More\")\n",
    "            #load_more_link.click()\n",
    "            driver.execute_script(\"arguments[0].click();\", load_more_link)\n",
    "            time.sleep(20)\n",
    "            j = j+1\n",
    "    except Exception as e:\n",
    "        print(\"Failed to click the 'Load More' link:\", e) \n",
    "\n",
    "    content = driver.page_source\n",
    "    soup = BeautifulSoup(content, \"lxml\")\n",
    "    art_links = []\n",
    "    data_links = soup.find_all(\"div\", {\"class\": \"imagebox\"})\n",
    "        # from each <a> element, extract the URL\n",
    "    for i in data_links:\n",
    "        ia = i.find('a')\n",
    "        if ia:\n",
    "            art_links.append(ia['href'])\n",
    "\n",
    "    driver.quit()\n",
    "    \n",
    "    df = pd.DataFrame(article_links, columns = [\"Links\"])\n",
    "    df.to_csv(\"Vishvas_News_english_links_june2020.csv\")\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb26bead",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_content(dt):\n",
    "    \n",
    "    for url in dt[\"Links\"]:\n",
    "\n",
    "        session = requests.Session()\n",
    "        retry = Retry(connect=3, backoff_factor=0.5)\n",
    "        adapter = HTTPAdapter(max_retries=retry)\n",
    "        session.mount('http://', adapter)\n",
    "        session.mount('https://', adapter)\n",
    "\n",
    "        response = session.get(url)\n",
    "\n",
    "        # Check if the request was successful\n",
    "        if response.status_code == 200:\n",
    "            # Parse the HTML content of the page using BeautifulSoup\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            meta_tags = soup.find_all('meta')\n",
    "            meta_features = {}\n",
    "            for tag in meta_tags:\n",
    "                if tag.get('property'):\n",
    "                    meta_features[tag['property']] = tag.get('content')\n",
    "            # title\n",
    "            title.append(meta_features['og:title'])\n",
    "\n",
    "            # Published_date\n",
    "            Published_date.append(meta_features['article:published_time'])\n",
    "\n",
    "            # top image\n",
    "            if 'og:image' in meta_features:\n",
    "                top_image.append(meta_features['og:image'])\n",
    "            else:\n",
    "                top_image.append(\" \")\n",
    "\n",
    "            # topic\n",
    "            topic.append(meta_features['article:section'])\n",
    "\n",
    "            # Content\n",
    "            data = soup.find_all('div', {'class':\"view-full\"})\n",
    "            if data:\n",
    "                extracted_content = []\n",
    "                for element in data:\n",
    "                    for el in element.find_all(['p', 'blockquote']):\n",
    "                        extracted_content.append(el.get_text())\n",
    "\n",
    "                full_content = '\\n'.join(extracted_content)\n",
    "\n",
    "            Content.append(full_content)\n",
    "\n",
    "            # Extract all links\n",
    "            view_full_section = soup.find('div', class_='view-full')\n",
    "            if view_full_section:\n",
    "                view_full_links = view_full_section.find_all('a')\n",
    "                linksss = []\n",
    "                for link in view_full_links:\n",
    "                    href = link.get('href')\n",
    "                    if link:\n",
    "                        linksss.append(link)\n",
    "\n",
    "            links_in_text.append(linksss)\n",
    "\n",
    "            # Claim\n",
    "            claim_section = soup.find('h2', text='Claim')\n",
    "            if claim_section:\n",
    "                claim_content = \"\"\n",
    "                for p_element in claim_section.find_next_siblings(['p', 'h2']):\n",
    "                    if p_element.name == 'h2' and p_element.text == 'Investigation':\n",
    "                        break\n",
    "                    claim_content += p_element.get_text() + \"\\n\"\n",
    "            else:\n",
    "                claim_content = \" \"\n",
    "\n",
    "            claim.append(claim_content)\n",
    "\n",
    "\n",
    "            # Investigation and conclusion\n",
    "            investigation_section = soup.find('h2', text='Investigation')\n",
    "            if investigation_section:\n",
    "                investigation_content = \"\"\n",
    "                for p_element in investigation_section.find_next_siblings(['p', 'h2']):\n",
    "                    if p_element.name == 'h2' and p_element.text == 'Know The Truthâ€¦ Spread Awareness':\n",
    "                        break\n",
    "                    investigation_content += p_element.get_text() + \"\\n\"\n",
    "            else:\n",
    "                investigation_content = \" \"\n",
    "\n",
    "            investigation.append(investigation_content)   \n",
    "\n",
    "\n",
    "            # image links\n",
    "            figure_elements = soup.find_all('figure', class_='wp-block-image')\n",
    "            images = []\n",
    "            for figure in figure_elements:\n",
    "                img = figure.find('img', {'data-src': True})\n",
    "                if img:\n",
    "                    data_src = img['data-src']\n",
    "                    images.append(data_src)\n",
    "\n",
    "            image_links.append(images)\n",
    "\n",
    "            # tags\n",
    "            tags_ul = soup.find('ul', class_='tags')\n",
    "            if tags_ul:\n",
    "                tag_items = tags_ul.find_all('li')\n",
    "\n",
    "                tagggs = [tag.text for tag in tag_items]\n",
    "            tags.append(tagggs)\n",
    "\n",
    "            # youtube links\n",
    "            iframes = soup.find_all('iframe')\n",
    "            y_links = []\n",
    "            for iframe in iframes:\n",
    "                if 'data-src' in iframe.attrs:\n",
    "                    youtube_link = iframe['data-src']\n",
    "                    y_links.append(youtube_link)\n",
    "\n",
    "            youtube_links.append(y_links)\n",
    "\n",
    "            linnnks.append(url)\n",
    "    df1 = pd.DataFrame({'link':linnnks, 'title': title, 'publish_date': Published_date, 'content': Content, 'claim': claim, 'investigation': investigation, 'Links_in_text':links_in_text,'top image': top_image, 'image links':image_links, 'video':youtube_links, 'tags':tags, 'topic': topic})\n",
    "    df1.to_csv('Vishvas_news_telugu_june2020.csv')\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "21cc1a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    title = []\n",
    "    Published_date = []\n",
    "    Content = []\n",
    "    claim = []\n",
    "    investigation = []\n",
    "    top_image = []\n",
    "    links_in_text = []\n",
    "    topic = []\n",
    "    image_links = []\n",
    "    youtube_links = []\n",
    "    tags = []\n",
    "    linnnks = []\n",
    "    get_links(90)\n",
    "    dataframe = pd.read_csv(\"Vishvas_News_telugu_links_june2020.csv\")\n",
    "    get_content(dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e8a1ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
