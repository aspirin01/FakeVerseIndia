{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ba8aabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time \n",
    "import pandas as pd\n",
    "import json\n",
    "import requests\n",
    "from pyquery import PyQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "188d8e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_links(page_num):\n",
    "    # Extraction of all links till june 2020\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "\n",
    "\n",
    "    #driver = webdriver.Chrome(service=service)\n",
    "    browser = webdriver.Chrome(service=service)\n",
    "    urls = []\n",
    "    counter = 2\n",
    "    articles_link = [\"Links\"]\n",
    "    # since the tops category has 7 pages, where link to each follows a specific pattern identified above, we can create links to them as following:\n",
    "    urls.append('https://hindi.boomlive.in/fact-check')\n",
    "    for i in range(2, page_num):\n",
    "        urls.append( 'https://hindi.boomlive.in/fact-check/' + str(counter))\n",
    "        counter += 1\n",
    "    # extracting links for products in each page\n",
    "    for url in urls:\n",
    "        # open the url\n",
    "        browser.get(url)\n",
    "        # purposeful wait time to allow the website to get fully loaded\n",
    "        time.sleep(4)\n",
    "        # get page content\n",
    "        content = browser.page_source\n",
    "        soup = BeautifulSoup(content, \"lxml\")\n",
    "        art_links = []\n",
    "        # extract all the anchor i.e., <a> elements with “thumb-link” class from the page\n",
    "        data_links = soup.find_all(\"a\", {\"class\":\"heading_link\"})\n",
    "        # from each <a> element, extract the URL\n",
    "        for i in data_links:\n",
    "            art_links.append(\"https://www.boomlive.in\" + i['href'])\n",
    "\n",
    "        articles_link.extend(art_links)\n",
    "        # purposeful wait time to avoid sending requests in quick succession\n",
    "        time.sleep(10)\n",
    "        \n",
    "    browser.quit()\n",
    "    np.savetxt(\"boomlive_hindi_links_june2020.csv\",\n",
    "        articles_link,\n",
    "        delimiter =\", \",\n",
    "        fmt ='% s')\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be9c0f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def restore_unicode(mangled):\n",
    "    return mangled.encode('latin1','ignore').decode('utf8', 'replace')\n",
    "def get_article_info(pq):\n",
    "    headline = pq(\"h1.entry-title\").text()\n",
    "    print(headline)\n",
    "    datestr = pq('span.date>span').text().split('Updated')[0]\n",
    "    print(datestr)\n",
    "    datestr = parse(datestr).astimezone(pytz.timezone('Asia/Calcutta')).strftime(\"%B %d, %Y\")\n",
    "    author_name = pq('a.author-name').text()\n",
    "    author_link = pq('a.author-name').attr['href']\n",
    "    article_info = {\n",
    "        \"headline\": restore_unicode(headline),\n",
    "        \"author\": restore_unicode(author_name),\n",
    "        \"author_link\": restore_unicode(author_link),\n",
    "        \"date_updated\": restore_unicode(datestr),\n",
    "    }\n",
    "    return article_info\n",
    "\n",
    "def get_article_content(pq):\n",
    "\n",
    "    content = {\n",
    "        \"text\": [],\n",
    "        \"video\": [],\n",
    "        \"image\": [],\n",
    "        \"tweet\": [],\n",
    "        \"facebook\": [],\n",
    "        \"instagram\": [],\n",
    "    }\n",
    "\n",
    "    ## text content\n",
    "    content['text'] = restore_unicode(pq('div.story').text())\n",
    "\n",
    "    ## images\n",
    "    images = pq.find('figure>img')\n",
    "    images += pq.find('.image-and-caption-wrapper>img')\n",
    "    images += pq.find('.single-featured-thumb-container>img')\n",
    "    images = list(dict.fromkeys(images))\n",
    "\n",
    "    for i in images:\n",
    "        if 'src' in i.attrib:\n",
    "            content[\"image\"].append(i.attrib[\"src\"])\n",
    "\n",
    "    ## video embed\n",
    "    video_embed = pq.find(\"video>source\")\n",
    "    for v in video_embed:\n",
    "        content[\"video\"].append(v.attrib[\"src\"])\n",
    "\n",
    "    video_yt = pq.find('iframe')  # video youtube\n",
    "\n",
    "    for v in video_yt:\n",
    "        if 'lazy' in v.attrib.get('class', ''):\n",
    "            continue\n",
    "        content[\"video\"].append(v.attrib[\"src\"])\n",
    "\n",
    "    fb = pq.find('.wp-block-embed-facebook>.fb-video')   # video fb\n",
    "    for f in fb:\n",
    "        content[\"facebook\"].append(f.attrib[\"data-href\"])\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c6d96a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_content(dt):\n",
    "    for url in dt[\"Links\"]:\n",
    "        #url = \"https://www.boomlive.in/fake-news/video-of-daring-bird-rescue-in-a-chopper-is-not-from-surat-6895\"\n",
    "        response = requests.get(str(url))\n",
    "\n",
    "        # Check if the request was successful\n",
    "        if response.status_code == 200:\n",
    "            # Parse the HTML content of the page using BeautifulSoup\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            meta_tags = soup.find_all('meta')\n",
    "            meta_features = {}\n",
    "            for tag in meta_tags:\n",
    "                if tag.get('name'):\n",
    "                    meta_features[tag['name']] = tag.get('content')\n",
    "            # title\n",
    "            title.append(meta_features['title'])\n",
    "\n",
    "            # Published_date\n",
    "            json_data = json.loads(soup.find(type=\"application/ld+json\").string, strict = False)\n",
    "            Published_date.append(json_data['datePublished'])\n",
    "\n",
    "            # top_image\n",
    "            top_image.append(meta_features['image'])\n",
    "\n",
    "            # image_links\n",
    "            images = []\n",
    "            for img in soup.findAll('img'):\n",
    "                temp = img.get('src')\n",
    "                if temp is not None:\n",
    "                    if temp[-4:] == \".jpg\":\n",
    "                        images.append(temp)\n",
    "\n",
    "            image_links.append(images)\n",
    "\n",
    "            html_text = response.content\n",
    "            pq = PyQuery(html_text)\n",
    "            content = get_article_content(pq)\n",
    "            Content.append(content['text'])\n",
    "    # storing into a dataframe\n",
    "    df = pd.DataFrame({'title': title, 'publish_date': Published_date, 'content': Content, 'top_image': top_image, 'image_links': image_links})\n",
    "    df[\"link\"] = dt[\"Links\"]\n",
    "    # saving the dataframe\n",
    "    df.to_csv('boom_gujarati.csv')\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "346b2742",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    title = []\n",
    "    headline = []\n",
    "    Published_date = []\n",
    "    Content = []\n",
    "    top_image = []\n",
    "    image_links = []\n",
    "    links_in_text = []\n",
    "    Tags = []\n",
    "    get_links(153)\n",
    "    dataframe = pd.read_csv(\"boomlive_gujarati_links_june2020.csv\")\n",
    "    get_content(dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243cc214",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bart1",
   "language": "python",
   "name": "bart1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
